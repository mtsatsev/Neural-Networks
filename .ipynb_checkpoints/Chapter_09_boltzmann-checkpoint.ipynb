{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author contributions\n",
    "Please fill out for each of the following parts who contributed to what:\n",
    "- Conceived ideas: \n",
    "- Performed math exercises: \n",
    "- Performed programming exercises:\n",
    "- Contributed to the overall final assignment: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9\n",
    "## Boltzmann machines\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Brightspace before the deadline (see Brightspace).\n",
    "    \n",
    "Learning goals:\n",
    "1. Implement a Boltzmann machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import urllib\n",
    "import skimage as ski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (unrestricted) Boltzmann machine is like a Hopfield network with stochastic state updates. The degree of randomness allows for investiations both in Hopfield networks and in how real associative memory may work (biological neurons are stochastic, too; and there are ideas that they make use of the irreducible noise). \n",
    "\n",
    "### Exercise 1: Flip-flop  (2 points)\n",
    "\n",
    "This is the **Boltzmann distribution** for a state $\\mathbf{x}$:\n",
    "\n",
    "$p(\\mathbf{x}) = \\frac{1}{Z} \\exp( \\frac{-E(\\mathbf{x})}{T} ) $\n",
    "\n",
    "with the state sum $Z$: \n",
    "\n",
    "$Z = \\sum_{\\mathbf{x}}  \\exp(\\frac{-E(\\mathbf{x})}{T})$\n",
    "\n",
    "\n",
    "Use it to derive the stable probability distribution of the **flip-flop** (2 units with bias $0.5$, connected with the identical weight $w_{12} = w_{21} = âˆ’1$). \n",
    "\n",
    "Assume the temperature $T=1$. Start by first computing $E(\\mathbf{x})$ for each possible state of the Hopfield network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "$\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement your own Boltzmann machine. As in the Hopfield network assignment, we will use an image and its mirrored version as the patterns the Boltzmann machine should learn. \n",
    "\n",
    "Note that we use the binary representation here, not the bipolar one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source image\n",
    "f = urllib.request.urlopen(\"https://homepages.cae.wisc.edu/~ece533/images/watch.png\")\n",
    "\n",
    "# Read the image\n",
    "x1 = mpimg.imread(f)\n",
    "x1 = ski.transform.resize(x1, (76, 102), mode=\"reflect\", anti_aliasing=True)\n",
    "\n",
    "# Make 2D and binary\n",
    "x1 = np.mean(x1, axis=2)\n",
    "x1[x1 < np.mean(x1.flatten())] =  0 # Black\n",
    "x1[x1 >= np.mean(x1.flatten())] = 1 # White\n",
    "x1.astype(\"int32\")\n",
    "\n",
    "# Make duplicate but mirrored second image\n",
    "x2 = np.fliplr(x1)\n",
    "\n",
    "# Flatten images\n",
    "sz = x1.shape\n",
    "X = np.stack((x1.flatten(), x2.flatten()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(X[:, 0].reshape(sz), cmap=\"gray\")\n",
    "ax[0].set_xticks([], [])\n",
    "ax[0].set_yticks([], [])\n",
    "ax[0].set_title(\"x1\")\n",
    "\n",
    "ax[1].imshow(X[:, 1].reshape(sz), cmap=\"gray\")\n",
    "ax[1].set_xticks([], [])\n",
    "ax[1].set_yticks([], [])\n",
    "ax[1].set_title(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: The sigmoid (1 point)\n",
    "Write a function `sigmoid(x)` that computes the *sigmoid activation function* $\\sigma(x)=\\frac{1}{1+exp(-x)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation function\n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Probabilities (1 point)\n",
    "\n",
    "Write a function `compute_probability(w, x, b, T)` that returns the probability that the state of node $x_i$ is set to $1$ ($0$ instead): \n",
    "\n",
    "$P(x_i=1)=\\sigma(\\frac{1}{T}(\\mathbf{w}_i^{\\top}\\mathbf{x}+b_i))$\n",
    "\n",
    "where $T$ is the temperature, $\\mathbf{w}_i$ the weights and $b_i$ the bias of node $i$; and $\\mathbf{x}$ the current state vector. $\\sigma$ is the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities\n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gibbs sampling  (2 points)\n",
    "Write a function `gibbs_sampling(w, b, temperature=1.0, n_gibbs=20, n_burnin=10)` that approximates the model distribution for training a Boltzmann machine via Gibbs sampling. The Boltzmann machine is given by the current weights `w` and biases `b`.\n",
    "\n",
    "1. Create an array `X` for saving the node states for each time step. Then initialize a random initial node state vector $\\mathbf{x}^{(1)}$ and save it in `X[:,0]`. These are the node states for $t=1$.\n",
    "1. For $t=2, ..., N$, where $N$ is the number of Gibbs sampling steps `n_gibbs`:\n",
    "    1. Compute the probability $P(x^{t+1}_i = 1)$ for all node states $x^{t}_i$ in `X[:,t]` (the previous state vector). \n",
    "    1. Determine the new binary state $x^{t+1}_i$ by drawing a random number between 0 and 1 using `np.random.rand()`, and comparing it with $P(x^{t+1}_i = 1)$. That is, depending on the probability for $1$, it should be set to $1$ or $0$ at time `t+1`. Save the new state in `X[i,t+1]`. \n",
    "1. Return `X` without the burn-in phase, that is without the first `n_burnin` samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling(w, b, temperature=1.0, n_gibbs=20, n_burnin=10):\n",
    "    n_nodes = w.shape[0]\n",
    "    \n",
    "    # Initialize states\n",
    "    X = np.zeros((n_nodes, n_gibbs))\n",
    "                 \n",
    "    # Initialize first state vector\n",
    "    ## Code here ##\n",
    "                 \n",
    "    # Loop over Gibbs samples\n",
    "    ## Code here ##\n",
    "                 \n",
    "        # Loop over nodes\n",
    "        ## Code here ##\n",
    "                 \n",
    "            # Compute probability for state 1\n",
    "            ## Code here ##\n",
    "                 \n",
    "            # Sample whether it should change to 1\n",
    "            ## Code here ##\n",
    "\n",
    "    # Discard burn-in\n",
    "    ## Code here ##\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Expectations  (1 point)\n",
    "Write a function `compute_expectations(X)` that computes the expectation (~mean over patterns / samples) of the partial derivatives for the weights and bias terms, given as: \n",
    "\n",
    "$$\\frac{\\partial J(x)}{\\partial w_{ij}}=-x_ix_j$$\n",
    "$$\\frac{\\partial J(x)}{\\partial b_i}=-x_i$$\n",
    "\n",
    "When running training you will compute these under the empirical and the model distribution (given as input `X`, which contains multiple patterns / sampling steps respectively). Note that if you use the dot product to compute $\\frac{\\partial J(x)}{\\partial w_{ij}}$ you need to divide by the number of patterns / samples `N` in `X`. Check for yourself (on paper, with a small example matrix `X` with 2 patterns) why this is so when you make use of the dot product to compute all $- x_i x_j$ at once. It will also help you arranging the dot product correctly. \n",
    "\n",
    "Hint: $I$ is the number of nodes. Then the partial derivative for the weight update `dw` should have the shape $(I, I)$. \n",
    "You also need to make sure that the diagonal is set so that the Boltzmann machine converges. You can complete this exercise in 3 lines of code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute expectations\n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Boltzmann training  (1 point)\n",
    "Now, with the components you wrote above, you can fill in the missing part in `boltzmann_train`. What is missing is updating the weights `w` and biases `b` with the information gathered from the empirical and model distribution, and the learning rate `eta`. Add the two lines performing this step below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_train(XE, temperature=1.0, eta=0.01, n_epochs=5, n_gibbs=20, n_burnin=10):\n",
    "    n_nodes, n_examples = XE.shape\n",
    "\n",
    "    # Initialize weights:\n",
    "    w = np.zeros((n_nodes, n_nodes))\n",
    "    \n",
    "    # Initialize biases:\n",
    "    b = np.zeros(n_nodes)\n",
    "    \n",
    "    # Compute expectations under the empirical distribution. As we compute this on \n",
    "    # the training patterns, we only need to do it once: \n",
    "    dE_dw, dE_db = compute_expectations(XE)     \n",
    "\n",
    "    # Loop over epochs: \n",
    "    for i_epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}.\".format(1 + i_epoch, n_epochs))\n",
    "        \n",
    "        # Gibbs sampling with the current model:\n",
    "        XM = gibbs_sampling(w, b, temperature, n_gibbs, n_burnin)\n",
    "\n",
    "        # Compute expectations under the model distribution: \n",
    "        dEM_dw, dEM_db = compute_expectations(XM)\n",
    "\n",
    "        # Update weights and biases\n",
    "        ## Code here ##\n",
    "    \n",
    "    # Force symmetry\n",
    "    w = (w + w.T) / 2\n",
    "    \n",
    "    print(\"Training done.\")\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Dreaming (1 point)\n",
    "\n",
    "Write a function `boltzmann_dream(w, b, temperature=1.0, n_epochs=20)` that you can use to sample images from a previously trained Boltzmann machine (given as `w` and `b`) that is left running on its own for `n_epochs`. This boils down to performing Gibbs sampling starting with a random state vector. Use your previously implemented functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boltzmann dreaming\n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 (1 point)\n",
    "\n",
    "**1.** Train your Boltzmann machine with `boltzmann_train` on the two images saved in `X` (the empirical distribution `XE`). Use $T=1$, 20 Gibbs sampling steps, a burn-in phase of 10, 3 training epochs, and a learning rate of $0.01$. Then let your trained Boltzmann machine dream for 20 epochs via `boltzmann_dream`. Plot the last 5 states as images. \n",
    "\n",
    "**2.** Now, change the temperature to $T=5$. Again, train the Boltzmann machine, let it run freely for 20 epochs and then plot the last 5 states it creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Boltzmann machine\n",
    "## Code here ##\n",
    "\n",
    "# Test Boltzmann machine\n",
    "## Code here ##\n",
    "\n",
    "# Plot last 5 samples\n",
    "## Code here ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Boltzmann machine\n",
    "## Code here ##\n",
    "\n",
    "# Test Boltzmann machine\n",
    "## Code here ##\n",
    "\n",
    "# Plot last 5 samples\n",
    "## Code here ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

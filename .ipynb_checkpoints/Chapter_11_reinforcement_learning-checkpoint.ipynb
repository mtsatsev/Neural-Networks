{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author contributions\n",
    "Please fill out for each of the following parts who contributed to what:\n",
    "- Conceived ideas: \n",
    "- Performed math exercises: \n",
    "- Performed programming exercises:\n",
    "- Contributed to the overall final assignment: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11\n",
    "## Reinforcement learning\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Brightspace before the deadline (see Brightspace).\n",
    "\n",
    "Learning goals:\n",
    "1. Reinforcement learning basics\n",
    "1. Tabular Q-Learning\n",
    "1. Neural network-based Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Notes on the architecture\n",
    "\n",
    "So far we have considered neural networks that learn in a supervised (provided with specific targets they should learn, e.g. classes) or unsupervised manner (not provided with targets, only data). \n",
    "\n",
    "Autonomous agents often learn from *rewards* (= positive or negative feedback quantities, i.e. this term includes rewards and punishments). In this set-up, the environment is considered a blackbox that returns a *reward* after an agent performs an *action*. In a typical setup rewards are the only thing agents know about their environment. This is the subject matter of reinforcement learning. \n",
    "\n",
    "In this assignment you will learn to implement an agent which can learn from its mistakes using reinforcement learning. \n",
    "\n",
    "We provide the environment `EvidenceEnv` that has a hidden world state (`0` or `1`). The task of the agent is to guess the right hidden world state, and the agent will receive a reward of `+1` or `-1` based on whether the guess was right or wrong respectively. The agent gets *evidence* from `EvidenceEnv` for being able to make an informed guess. The generated evidence is *equal* to the true state with probability $p$, and different from the true state with probability $1-p$. \n",
    "\n",
    "Please read the code and make sure to find as much of what we described inside it as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceEnv(object):\n",
    "    \"\"\"\n",
    "    Very simple task which only requires evaluating present evidence and does not require evidence integration.\n",
    "    The actor gets a reward when it correctly decides on the ground truth. Ground truth 0/1 influences \n",
    "    probabilistically the number of 0s or 1s in the evidence observations. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=1, p=0.8):\n",
    "        \"\"\"\n",
    "        Initializes the environment.\n",
    "        \n",
    "        Args:\n",
    "            n = [int] number of pieces of evidence\n",
    "            p = [flt] probability of emitting matching evidence\n",
    "        \"\"\"\n",
    "        self.n_obs = n\n",
    "        self.p = p\n",
    "        self.n_action = 2\n",
    "        self._state = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets state and generates new observations. \n",
    "\n",
    "        Returns:\n",
    "            observation = [int] new initial evidence\n",
    "        \"\"\"\n",
    "        self._state = np.random.choice(2)  # generate new hidden state\n",
    "        return self.observe()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes and carries out the agent's action (the guess). Then proceeds to create a new state\n",
    "        and new evidence. Returns evidence for new state and reward for guess on old state. \n",
    "\n",
    "        Args:\n",
    "            action = [int] the action\n",
    "        Returns:\n",
    "            observation = [int] the observation\n",
    "            reward      = [int] the reward\n",
    "        \"\"\"\n",
    "        # reward is +1 for correct decision and -1 for incorrect decision: \n",
    "        reward = (2 * (action == self._state) - 1)  \n",
    "        new_observations = self.reset()\n",
    "        return new_observations, reward\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"\n",
    "        Generate n_obs pieces of evidence for current state. \n",
    "\n",
    "        Returns:\n",
    "            observation = [int] the observation\n",
    "        \"\"\"\n",
    "        P = [self.p, 1 - self.p] if self._state == 0 else [1 - self.p, self.p]\n",
    "        return np.random.choice(2, self.n_obs, True, P).astype('float32').reshape([1, self.n_obs])[0]\n",
    "\n",
    "    # helper functions\n",
    "    def asint(self, observations):\n",
    "        \"\"\"\n",
    "        Represent list of 0,1 observations as an integer number.\n",
    "\n",
    "        Args:\n",
    "            observations = [list] binary array\n",
    "        Returns:\n",
    "            observations = [int] integer representation\n",
    "        \"\"\"\n",
    "        return int(sum(2**i*b for i, b in enumerate(observations)))\n",
    "\n",
    "    def asbinary(self, i, b_len):\n",
    "        \"\"\"\n",
    "        Represent an integer as a 0,1 array.\n",
    "        \n",
    "        Args\n",
    "            i     = [int] integer number\n",
    "            b_len = [int] length of binary array\n",
    "        Returns:\n",
    "            b = [list] binary representation\n",
    "        \"\"\"\n",
    "        b = [int(x) for x in list('{0:0b}'.format(i))]\n",
    "        b = [0 for i in range(b_len - len(b))] + b\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, agent, epsilon=0, n_iter=1000):\n",
    "    \n",
    "    # Reset environment and agent, create first observation\n",
    "    obs_old = env.reset()\n",
    "    reward = None\n",
    "    \n",
    "    # Loop over iterations\n",
    "    R = np.empty(n_iter)  # keep track of rewards\n",
    "    for i_iter in range(n_iter):\n",
    "\n",
    "        # Choose an action\n",
    "        if i_iter > 0 and np.random.rand() < epsilon:  # epsilon determines whether we go for exploration or exploitation\n",
    "            action = np.random.choice(env.n_action)  # random actions to explore environment (exploration)\n",
    "        else:\n",
    "            action = agent.act(obs_old) # strictly follow currently learned behaviour (exploitation)\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            raise Exception(\"action should be an integer, not a torch.Tensor!\")\n",
    "\n",
    "        # Perform the selected action, get the reward, and a new observation for the next round\n",
    "        obs_new, reward = env.step(action)\n",
    "\n",
    "        # Adjust agent parameters (training step)\n",
    "        agent.train(action, obs_old, reward, obs_new)\n",
    "\n",
    "        # Update observation variable\n",
    "        obs_old = obs_new\n",
    "\n",
    "        # Track the rewards\n",
    "        R[i_iter] = reward\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = EvidenceEnv(n=2, p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Define random agent  (1 point)\n",
    "Define a random agent that just makes a random guess (random action, selected from uniform distribution). The function `.act` takes an observation, but you can ignore it here. \n",
    "\n",
    "Note that the initialized agent object will have a reference to the environment, being able to probe it from within the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"\n",
    "    Implements a random agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes an agent with an environment. \n",
    "        \n",
    "        Args:\n",
    "            env = [obj] the environment\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Act based on observation. \n",
    "        \n",
    "        Args:\n",
    "            observation = [int] the observation\n",
    "        Returns:\n",
    "            action = [int] the action\n",
    "        \"\"\"\n",
    "        ### Write code here. ###\n",
    "        return action\n",
    "\n",
    "    def train(self, action, obs_old, reward, obs_new):\n",
    "        \"\"\"\n",
    "        Trains the agent.\n",
    "        \n",
    "        Args:\n",
    "            action  = [int] the action\n",
    "            obs_old = [int] observation before action\n",
    "            reward  = [int] reward after action\n",
    "            obs_new = [int] observation after action\n",
    "        \"\"\"\n",
    "        pass  # not implemented in this exercise, nothing to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Run random agent (1 points)\n",
    "\n",
    "1. Initialize your random agent with the already initialized environment `env`.\n",
    "1. Let the agent run with the environment with `run_agent`. Use a reasonable but small `epsilon`, e.g. `0.05`. \n",
    "1. Observe its behaviour by plotting the cumulative reward (cumulative sum of the reward) across 5000 iterations. \n",
    "1. Is it doing something rational?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "### Write code here ###\n",
    "\n",
    "# Put the agent in the environment\n",
    "### Write code here ###\n",
    "\n",
    "# Plot cumulative reward\n",
    "### Write code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Define tabular Q-learning (2.5 points)\n",
    "We proceed to implement reinforcement learning via standard tabular Q-learning. The goal of Q-learning is to estimate the value of taking an action $a$ in an state $s$ (the observed state, not the hidden state). The agent selects that action which maximizes the value given an input state (observations). If you did not encounter reinforcement learning in the AI programme before, read the following blog up to and including the section Q-learning.\n",
    "http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/\n",
    "\n",
    "Implement the tabular Q-learning algorithm by filling in the two missing lines of code below. Choosing the action means selecting $a = argmax_a Q(s, a)$. Updating the Q-table is merely applying the Q-learning update step together with the learning rate $\\eta$ (`self.lr`), and discount factor $\\gamma$ (`self.y`); then saving the resulting new value $Q(s,a)$ in the lookup table. \n",
    "\n",
    "Using the Bellman equation: \n",
    "\n",
    "$Q(s, a) = Q(s, a) + \\eta (r + \\gamma Q(s', a') - Q(s, a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQAgent(object):\n",
    "    \"\"\"\n",
    "    Implements Tabular Q agent.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initializes an agent.\n",
    "        \n",
    "        Args:\n",
    "            env = [obj] the environment\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        n_obs = env.asint(np.array([1 for i in range(env.n_obs)]))+1\n",
    "\n",
    "        # Initialize the Q-table (s,a) with all zeros\n",
    "        self.Q = np.zeros([n_obs, env.n_action])\n",
    "\n",
    "        # Set learning parameters\n",
    "        self.lr = 1.0\n",
    "        self.y = 0.95  # gamma\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Act based on observation.\n",
    "        \n",
    "        Args:\n",
    "            observation = [int] the observation\n",
    "        Returns:\n",
    "            action = [int] the action\n",
    "        \"\"\"\n",
    "\n",
    "        # Transform observation to integer representation\n",
    "        s = self.env.asint(observation)\n",
    "\n",
    "        # Choose an action by greedily picking from the Q-table (corresponding entry for s and a)\n",
    "        ### Write code here ###\n",
    "\n",
    "        return a\n",
    "\n",
    "    def train(self, a, old_obs, r, new_obs):\n",
    "        \"\"\"\n",
    "        Trains the agent.\n",
    "        \n",
    "        Args:\n",
    "            action  = [int] the action\n",
    "            obs_old = [int] observation before action\n",
    "            reward  = [int] reward after action\n",
    "            obs_new = [int] observation after action\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.env.asint(old_obs)\n",
    "        s1 = self.env.asint(new_obs)\n",
    "\n",
    "        # Update the Q-table\n",
    "        ### Write code here ###\n",
    "        \n",
    "    def get_Q(self):\n",
    "        \"\"\"\n",
    "        Returns the Q table.\n",
    "        \n",
    "        Returns:\n",
    "            Q = [n_obs, n_actions] the Q table\n",
    "        \"\"\"\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Run Tabular Q-agent (1.5 points)\n",
    "1. Just like for the random agent, initialize a tabular Q-agent with the environment `env`.\n",
    "1. Run it for 5000 iterations. Use the same epsilon as before. \n",
    "1. Observe its behaviour by plotting the cumulative reward. \n",
    "1. Also plot the cumulative reward of the random agent for comparison into the same figure. Is the Q-agent doing better?\n",
    "1. Also plot $Q(s,a)$ (the Q-table) before and after learning via `plt.imshow()`. Call `agent.get_Q()` to get the Q-table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "### Write code here ###\n",
    "\n",
    "# Save initial Q table\n",
    "### Write code here ###\n",
    "\n",
    "# Run agent\n",
    "### Write code here ###\n",
    "\n",
    "# Plot cumulative reward\n",
    "### Write code here ###\n",
    "\n",
    "# Plot Q table before and after training\n",
    "### Write code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Neural Network-based Q-learning (2.5 points)\n",
    "\n",
    "1. Ideally we want to get rid of the tabular representation. Read the rest of the blog post. Why is this lookup table impractical?\n",
    "\n",
    "We will implement an agent which uses an MLP that takes observations and learns to output the Q value for all possible actions. This is done by backpropagation on the loss defined by the sum squared difference between the predicted and desired Q values. For simplicity we backpropagate per example.\n",
    "\n",
    "1. Implement an MLP in chainer with two linear weight layers. It should use a ReLU activation function at the first layer and a linear (identity) activation at the second layer, producing the output `y`.\n",
    "1. Fill in the missing lines in the `act()` and `train()` functions. It is quite similar to the Tabular Q learning algorithm, although you use the predicted q-values. For a more in depth explanation, read the last sections in http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (example): The action-value function is estimated separately and from scratch for each state-action pair, without any generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        ## Code here ##\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Code here ##\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralQAgent(object):\n",
    "    \"\"\"\n",
    "    Implements a Neural Q agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, n_hidden=2, lr=0.01, y=0.95):\n",
    "        \"\"\"\n",
    "        Initializes an agent.\n",
    "        \n",
    "        Args:\n",
    "            env = [obj] the environment\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set environment\n",
    "        self.env = env\n",
    "        self.n_obs = env.asint(np.array([1 for i in range(env.n_obs)]))+1\n",
    "        \n",
    "        # Set learning parameters\n",
    "        self.lr = lr\n",
    "        self.y = y\n",
    "        \n",
    "        # Set model\n",
    "        self.model = MLP(n_input=2, n_hidden=n_hidden, n_output=env.n_action)\n",
    "        \n",
    "        # Set loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Setup an optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Act based on observation.\n",
    "        \n",
    "        Args:\n",
    "            obs = [int] the observation\n",
    "        Returns:\n",
    "            action = [int] the action\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert to tensor\n",
    "        obs = torch.from_numpy(np.atleast_2d(obs))\n",
    "        \n",
    "        # Apply q-learning neural network to get q-value estimation\n",
    "        ## Code here ##\n",
    "\n",
    "        # Choose an action by greedily picking from Q table\n",
    "        ## Code here ##\n",
    "\n",
    "        # Keep track of selected q value (needed for training)\n",
    "        ## Code here ##\n",
    "\n",
    "        return a\n",
    "\n",
    "    def train(self, a, old_obs, r, new_obs):\n",
    "        \"\"\"\n",
    "        Trains the agent.\n",
    "        \n",
    "        Args:\n",
    "            action  = [int] the action\n",
    "            obs_old = [int] observation before action\n",
    "            reward  = [int] reward after action\n",
    "            obs_new = [int] observation after action\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert to tensor\n",
    "        new_obs = torch.from_numpy(np.atleast_2d(new_obs))\n",
    "        \n",
    "        # Compute the 'right' (target) q-value qtarget for this state (via the Bellman equation, see blog post)\n",
    "        ## Code here ##\n",
    "        \n",
    "        # Update\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(self._qvalue[0], qtarget)\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_Q(self):\n",
    "        \"\"\"\n",
    "        Returns the Q table.\n",
    "        \n",
    "        Returns:\n",
    "            Q = [n_obs, n_actions] the Q table\n",
    "        \"\"\"\n",
    "        Q = np.zeros([self.n_obs, self.env.n_action])\n",
    "        for i_obs in range(self.n_obs):\n",
    "            obs = np.array(self.env.asbinary(i_obs, self.env.n_obs)).astype(\"float32\")\n",
    "            obs = torch.from_numpy(np.atleast_2d(obs))\n",
    "            qvalues = self.model(obs)\n",
    "            Q[i_obs, :] = qvalues.data.detach().numpy()\n",
    "        return Q\n",
    "    \n",
    "    def reset_state(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Run the NeuralQAgent (1.5 point)\n",
    "\n",
    "1. Initialize `NeuralQAgent` with the environment and let it run for 5000 epochs. Use the same epsilon as before. \n",
    "1. Observe its behaviour by plotting the cumulative reward. Plot the reward of the random and the tabular Q-agents for comparison.\n",
    "1. Also plot the learned Q-values before and afterer learning again. You get them by calling `agent.get_Q()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was only a first step in reinforcement learing, and Q-learning with neural networks. Note further that we did not implement the various training tricks that were mentioned in the lecture. Also, we are dealing with an environment in which the reward does not depend on the past. More general setups require that we deal with past influences as well. \n",
    "\n",
    "If you want to go further:  The framework we defined here adheres to OpenAI `gym`â€™s definitions: https://gym.openai.com You can use your implementation as a basis for solving state-of-the-art reinforcement learning tasks, such as gait learning or Atari games. Other interesting places to go further: \n",
    "\n",
    "* **PsychLab**: Solving classical psychology experiments with current AI to research artificial and biological intelligence and their differences: https://deepmind.com/blog/open-sourcing-psychlab/\n",
    "* **OpenAI Retro Contest**: Current transfer learning contest: https://contest.openai.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
